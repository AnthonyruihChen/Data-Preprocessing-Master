#==============================================================================
#
# PROGRAM OVERVIEW :
# 
# Parse Embedded JSON_0614.py is a program that reads in a file containing 
# JSON data and extracts said data into a wide table format, in which the 
# values of JSON object become extended into a single array.
#
#
# INPUTS : 
# 
# This program requires a handful of inputs to be specified:
# 
# 1. The paths to the consolidated input files.
# 2. The path to the desired output file. 
# 3. Path to a file that specifies information on structure of the JSON data
# which is generated by the program GetFlattened_JSON Name_0614.py
# 
# 
# OUTPUTS : 
# 
# The output generated by this program is a regular table format for the data 
# in which non-JSON and JSON values associated with specific fields become the
# elements of an array that is a single record in the dataset.
#
# ASSUMPTIONS : 
# 
# Program contains certain assumptions on the structure of the input data, 
# such as the field position of the JSON data.
# 
# This program contains many rules by which it applies differennt treaments 
# to fields and patterns and that indicate significant features of the data's
# structure and it was built/adapted for a specific set of circumstances for a 
# specific batch of consolidated batch of audit logs. 
# 
# These rules are too many to document for present purposes and it will suffice
# to acknowledge that they exist. 
# 
# Despite containing said rules, this program is generic in the sense that 
# it demonstrates a general approach to the extraction of JSON data that was 
# on this engagement. 
# 
#
#==============================================================================


import pandas as pd
import numpy as np
import json
import re
import csv
from pandas.io.json import json_normalize
from itertools import islice


def flatten_json(y):
    out = {}
    
    def bracketsplit(s):
        parts = []
        bracket_level = 0
        current = []
        # trick to remove special-case of trailing chars
        for c in (s + "~"):
            if c == "~" and bracket_level == 0:
                parts.append("".join(current))
                current = []
            else:
                if (c == "{") | (c == "(")  :
                    bracket_level += 1
                elif (c == "}") | (c == ")") :
                    bracket_level -= 1
                current.append(c.replace('~', ' -'))
        parts.append("".join(current)[:-1]) #if there are any stagglers append them to the list
        return parts   
    
    def flatten(x, name=''):
            
        ##Special cases for ModifiedProperties
        if (name.startswith('ModifiedProperties')):
            out['ModifiedProperties'] = x
        else:
            if ( name.startswith('ExtendedProperties')) | (name.startswith('Parameters')) | (name.startswith('NonPIIParameters')):
                if type(x) is list:
                    i = 0
                    for a in x:
                        if type(a) is dict:
                            paramname = name + a['Name']
                            out[paramname] = a['Value']
                        i += 1
                else:
                    if ((str(x).replace('"','').startswith('-'))):
                    #file2.write(str(x)+'\r\n')
                        
                        stripped = re.sub(r'^"{0,1}-', "", str(x))
                        split = re.sub(r" -(?=\w)","~", stripped)
                        split = bracketsplit(split)
                        split2 = [i.split(' ',1) for i in split]
                        split3 = [s for s in split2 if len(s) == 2]
                        out.update({name + d[0]: d[1] for d in split3})
                        out[name[:-1]] = x
            else:
                if type(x) is dict:
                    for element in x:
                        flatten(x[element], name + element + '_')
                elif type(x) is list:

                    i = 0
                    for a in x:
                        flatten(a, name + str(i) + '_')
                        i += 1
                else: 

                    out[name[:-1]] = x

    flatten(y)
    return out


file = open('//ussltcher0118.us.deloitte.com/Projects/Windham/New Team Folder/Work Files/Daisy/Parse_JSON/JSON_keys_0614.txt','r')
json_field_name = []
for line in file:
    json_field_name.append(line[:-1])

file.close()
num_JSONfield = len(json_field_name)
print (str(num_JSONfield) + ' json fields')

index = range(0,len(json_field_name))
json_field_dict = dict(zip(json_field_name,index))
# print (json_field_dict)
column_name = ['column' + str(i) for i in range(0,15)]
column_name = column_name + json_field_name

count = 0
count_clean = 0
count_jserr = 0
count_colerr = 0

jserrfile =  open('//ussltcher0118.us.deloitte.com/Projects/Windham/New Team Folder/Work Files/Daisy/Parse_JSON/JSONerror_0614.csv','w',encoding='utf-16')
combinefile = open('//ussltcher0118.us.deloitte.com/Projects/Windham/New Team Folder/Work Files/Daisy/Parse_JSON/AdminAudit_clean_JScombined_0614.csv','w',encoding='utf-16')
csvCombined = csv.writer(combinefile, quoting=csv.QUOTE_ALL, lineterminator='\n', delimiter='|')

with open('//ussltcher0118.us.deloitte.com/Projects/Windham/New Team Folder/01_Raw Data/20170602_Various/AdminAudit_2017-1-30_2017-2-28.tsv',encoding = 'utf-8') as file:
    reader = csv.reader(file, delimiter = '\t')
    header = next(reader)
    column_name = header + ['DT_ID_Orig', 'DT_Source'] + json_field_name
    
    csvCombined.writerow(column_name)
    for csvline in reader:
        json_output = ['']*num_JSONfield
        count = count + 1
            # line[1:-2]]
        try:
            csvline[6] = json.loads(csvline[6])
            if len(csvline) > 14:
                csvline[7] = csvline[7]+','+csvline[8]
                csvline = csvline[:8]+csvline[9:]
                print("after fixing, number of columns are "+ str(len(csvline)))
                count_colerr += 1
            csvline.append(count)
            csvline.append('AdminAudit_2017-1-30_2017-2-28.tsv')
            json_field = csvline[6]
            jsonDict = flatten_json(csvline[6])
            if 'OriginatingServer' in jsonDict:
                if jsonDict['OriginatingServer'][-2:] == '\r\n':
                    jsonDict['OriginatingServer'] = jsonDict['OriginatingServer'][:-2]
            for key, value in jsonDict.items():
                idx = json_field_dict[key]
                json_output[idx] = value
            csvline[6] = json.dumps(csvline[6])
            combined = csvline +json_output
            csvCombined.writerow(combined)
            count_clean += 1
        except:
            jserrfile.write('|'.join(csvline)+'\r\n')
            count_jserr += 1
        if (count % 10000 == 0):
            print ("Processing:" + str(count))
print(str(count)+" records loaded")
print(str(count_clean)+" clean records loaded")
print(str(count_colerr)+" column spillage error messages")
print(str(count_jserr)+" json error messages")


with open(r'\\ussltcher0118.us.deloitte.com\Projects\Windham\New Team Folder\01_Raw Data\Audit\AdminAudit_unscrubbed.csv',encoding='utf-16') as file:
    for line in file:
            # print (line)
        json_output = ['']*num_JSONfield
        count = count + 1
        reader = csv.reader([line[1:-2]])
        for csvline in reader:
            try:
                csvline[6] = json.loads(csvline[6])
                if len(csvline) > 14:
                    csvline[7] = csvline[7]+','+csvline[8]
                    csvline = csvline[:8]+csvline[9:]
                    print("after fixing, number of columns are "+ str(len(csvline)))
                    count_colerr += 1
                csvline.append(count)
                csvline.append('AdminAudit_unscrubbed.csv')
                json_field = csvline[6]
                jsonDict = flatten_json(csvline[6])
                print (jsonDict)
                if 'OriginatingServer' in jsonDict:
                    if jsonDict['OriginatingServer'][-2:] == '\r\n':
                        jsonDict['OriginatingServer'] = jsonDict['OriginatingServer'][:-2]
                for key, value in jsonDict.items():
                    idx = json_field_dict[key]
                    json_output[idx] = value
                csvline[6] = json.dumps(csvline[6])
                combined = csvline +json_output
                csvCombined.writerow(combined)
                count_clean += 1
            except:
                jserrfile.write('|'.join(csvline)+'\r\n')
                count_jserr += 1
        if (count % 10000 == 0):
            print ("Processing:" + str(count))
print(str(count)+" records loaded")
print(str(count_clean)+" clean records loaded")
print(str(count_colerr)+" column spillage error messages")
print(str(count_jserr)+" json error messages")

jserrfile.close()
combinefile.close()

